These examples give a quick overview of the Spark API.
Spark is built on the concept of distributed datasets,
which contain arbitrary Java or Python objects. 
You create a dataset from external data, then apply 
parallel operations to it. The building block of the 
Spark API is its RDD API. In the RDD API, there are two 
types of operations: transformations, which define a new 
dataset based on previous ones, and actions, which kick off a
are provided, e.g. DataFrame API and Machine Learning API. These high level API
s provide a concise way to conduct certain data operations. In this page, we will show 
examples using RDD API as well as examples using high level APIs.
In Spark, a DataFrame is a distributed collection of data organized in
to named columns. Users can use DataFrame API to perform various relational 
operations on both external data sources and Spark’s built-in distributed coll
ections without providing specific procedures for processing data. Also, programs b
ased on DataFrame API will be automatically optimized by Spark’s built-in optimizer, 
Catalyst.